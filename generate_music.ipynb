{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gn0pE_2cZ8h4"
      },
      "source": [
        "# Packages"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-jhmQabiiFQ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0XhV0cwYTQA"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JP5XkAvYZeMX"
      },
      "source": [
        "# Base setting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wuhjPQiVoZrT"
      },
      "outputs": [],
      "source": [
        "year = ['2004','2006','2008','2009','2011','2013','2014','2015','2017','2018']\n",
        "BATCH= 10\n",
        "max_sequence_length = 128\n",
        "vocab_size = 128\n",
        "drop_prob = 0.1\n",
        "num_layers=  5\n",
        "num_heads = 8\n",
        "ffn_hidden = 1024\n",
        "d_model = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ia2rjz3ZhN4"
      },
      "outputs": [],
      "source": [
        "def make_sort_indx(x,batch_size):\n",
        "    '''\n",
        "    sort tensor\n",
        "    [[0,x,0],[1,x,0],....,[m-1,x,n],[m,x,n]]\n",
        "    to\n",
        "    [[0,x,0],[0,x,1],...[m,x,n-1],[m,x,n]]\n",
        "    '''\n",
        "    t1 = tf.range(batch_size)\n",
        "    t2 = tf.range(d_model)\n",
        "\n",
        "    g1,g2 = tf.meshgrid(t1,t2)\n",
        "\n",
        "    r = tf.stack([g1,tf.fill(tf.shape(g1),x),g2],axis=-1)\n",
        "\n",
        "    r_flat = tf.reshape(r,[-1,3])\n",
        "\n",
        "    indices = []\n",
        "    for i in range(batch_size):\n",
        "        indices = indices + [k for k in range(i,batch_size*d_model,batch_size)]\n",
        "    # print('r_flat',r_flat)\n",
        "    results = tf.gather(r_flat,indices)\n",
        "    # print('results',results)\n",
        "    results = results.numpy()\n",
        "    results = results.tolist()\n",
        "    return results\n",
        "\n",
        "def indices_dict(batch_size):\n",
        "\n",
        "    '''\n",
        "    依照[m,x,n] 中 x 的不同給予不同的index\n",
        "    '''\n",
        "    index_list = list()\n",
        "    for i in range(max_sequence_length):\n",
        "        index_list.append(make_sort_indx(i,batch_size))\n",
        "\n",
        "    return np.array(index_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJHt3dJHp4eM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c1988a78-690b-40c4-c863-a18e898b7543"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAtfm5hnYT15"
      },
      "source": [
        "# Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OLHHGswFYsMp"
      },
      "outputs": [],
      "source": [
        "from google.colab import files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8k6JGCbcMqyE"
      },
      "outputs": [],
      "source": [
        "sequences = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kLYS6aGqr_RI"
      },
      "outputs": [],
      "source": [
        "# 載入事先準備好的npy檔\n",
        "for y in year:\n",
        "  data = np.load(f'./drive/MyDrive/music_note_dataset/note_{y}.npy')\n",
        "  sequences.append(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K59HByxJOq7Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce6c114c-f275-4460-b173-cd1ea8a9a5b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[71, 71, 55, ..., 76, 50, 66],\n",
              "       [67, 64, 54, ..., 74, 55, 73],\n",
              "       [74, 57, 76, ..., 73, 74, 55],\n",
              "       ...,\n",
              "       [92, 92, 53, ..., 53, 47, 55],\n",
              "       [79, 53, 86, ..., 43, 36, 48],\n",
              "       [67, 43, 36, ..., 53, 43, 53]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "sequences[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g1aAFSY8M2WS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9d335e5e-ea8d-4899-8131-1a85381c0f64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(12674, 128),\n",
              " (13454, 128),\n",
              " (8870, 128),\n",
              " (12260, 128),\n",
              " (9480, 128),\n",
              " (8472, 128),\n",
              " (12930, 128),\n",
              " (8264, 128),\n",
              " (9374, 128),\n",
              " (13596, 128)]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "[i.shape for i in sequences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVYpBGBmNJ82"
      },
      "outputs": [],
      "source": [
        "X = np.concatenate(tuple(i for i in sequences),axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlbFnt7EOtpV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d3888a-68e8-4561-a1ae-999730185760"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([71, 71, 55, 71, 59, 55, 59, 62, 62, 72, 71, 67, 72, 57, 74, 67, 72,\n",
              "       74, 74, 72, 72, 67, 74, 67, 66, 66, 72, 57, 71, 64, 71, 72, 72, 74,\n",
              "       59, 64, 62, 66, 62, 67, 66, 59, 74, 71, 71, 74, 78, 74, 59, 59, 79,\n",
              "       78, 76, 60, 79, 60, 76, 79, 74, 79, 59, 74, 79, 59, 72, 79, 57, 72,\n",
              "       71, 71, 72, 66, 57, 67, 81, 72, 66, 72, 55, 81, 72, 71, 71, 69, 59,\n",
              "       59, 69, 67, 55, 71, 62, 67, 54, 62, 54, 67, 52, 67, 52, 50, 71, 50,\n",
              "       49, 64, 76, 64, 76, 69, 52, 52, 69, 76, 57, 67, 76, 49, 67, 66, 66,\n",
              "       49, 67, 49, 76, 67, 67, 76, 50, 66], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "X[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EGDi-UXNbbe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a000d43-8e72-4f1c-a49e-a3588c820ce4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(109374, 128)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GPAmLk0KsJjf"
      },
      "outputs": [],
      "source": [
        "X = X.reshape((X.shape[0],X.shape[1]))\n",
        "X = tf.constant(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8rjb-_MYVFv"
      },
      "source": [
        "# Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gigap7BgeviH"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import random\n",
        "from tensorflow.keras import Model,Sequential,activations\n",
        "from tensorflow.keras.layers import Dense,Dropout,Input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5mOrYUnQxar"
      },
      "source": [
        "### PositionalEncoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZ0hRNy7Q1kz"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,d_model,max_sequence_length):\n",
        "    super().__init__()\n",
        "    self.max_sequence_length = max_sequence_length\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.weights_var = self.add_weight(\n",
        "      shape=(max_sequence_length,d_model),\n",
        "      initializer='glorot_uniform',\n",
        "      trainable=True,\n",
        "      name='positional_encoding_weights'\n",
        "    )\n",
        "\n",
        "  def call(self):\n",
        "    even_i = tf.range(start=0,limit=self.d_model,delta=2,dtype=tf.float32)\n",
        "    denominator = tf.pow(10000.0,even_i/self.d_model)\n",
        "    # denominator = tf\n",
        "    position = tf.reshape(tf.range(self.max_sequence_length,dtype=tf.float32),[self.max_sequence_length,1])\n",
        "    even_PE = tf.math.sin(position/denominator)\n",
        "    odd_PE = tf.math.cos(position/denominator)\n",
        "    stacked = tf.stack([even_PE,odd_PE],axis=2)\n",
        "    # print(stacked)\n",
        "    PE = tf.reshape(stacked, [stacked.shape[0],stacked.shape[1]*stacked.shape[2]])\n",
        "    # print(PE)\n",
        "    PE = tf.cast(PE,dtype=tf.float32)\n",
        "    return PE * self.weights_var"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3c2hck8QjC4"
      },
      "source": [
        "### Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pTUVeqoQlj5"
      },
      "outputs": [],
      "source": [
        "class SequenceEmbedding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,max_sequence_length,d_model,lan_to_index,START_TOKEN=None,END_TOKEN=None,PADDING_TOKEN=None):\n",
        "    super().__init__()\n",
        "    time_of_set = 4\n",
        "    # self.vocab_size = len(lan_to_index)\n",
        "    self.max_sequence_length = max_sequence_length\n",
        "    self.embedding = TemporalEmbedding(max_sequence_length=max_sequence_length,d_model=d_model)\n",
        "    self.lan_to_index = lan_to_index\n",
        "    self.position_encoder =PositionalEncoding(d_model,max_sequence_length)\n",
        "    self.dropout = Dropout(0.1)\n",
        "    self.START_TOKEN = START_TOKEN\n",
        "    self.END_TOKEN = END_TOKEN\n",
        "    self.PADDING_TOKEN = PADDING_TOKEN\n",
        "\n",
        "\n",
        "  def call(self,x,batch_size): # sentence\n",
        "    # print('se_input',x)\n",
        "    x = self.embedding(x,batch_size)\n",
        "    # print('se0',x)\n",
        "    pos = self.position_encoder.call()\n",
        "    # print('se1',pos)\n",
        "    x = self.dropout(x+pos)\n",
        "    # print('se2',x)\n",
        "    # x = tf.reshape(x,[x.shape[0],x.shape[1]])# 從二維轉成三維(1,128,512)\n",
        "    return x\n",
        "\n",
        "# def get_temporal_embeddings(position, d_model):\n",
        "#     angle_rates = 1 / np.power(10000, (2 * (np.arange(d_model) // 2)) / np.float32(d_model))\n",
        "#     return tf.constant(position * angle_rates,dtype=tf.float32)\n",
        "def get_temporal_embeddings(position, d_model):\n",
        "    angle_rates = 1 / tf.pow(10000.0, (2.0 * (tf.range(d_model, dtype=tf.float32) // 2.0) / tf.cast(d_model, dtype=tf.float32)))\n",
        "    return position * angle_rates\n",
        "\n",
        "class TemporalEmbedding(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,max_sequence_length,d_model):\n",
        "    super(TemporalEmbedding,self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.max_sequence_length = max_sequence_length\n",
        "    # print(f'TemporalEmbedding max_seq_len: {self.max_sequence_length},d_model: {self.d_model}')\n",
        "    # 設定trainable\n",
        "    # self.trainable = False\n",
        "    self.temporal_weights = self.add_weight(\n",
        "      shape=(d_model,max_sequence_length ),\n",
        "      initializer='glorot_uniform',\n",
        "      trainable=True,\n",
        "      name='temporal_weights'\n",
        "      )\n",
        "  def call(self,inputs,batch_size):\n",
        "    # inputs: N x d_model(every input sequence length)\n",
        "    # output: N x max_seq_length x d_model\n",
        "    inputs = tf.cast(inputs,dtype=tf.float32)\n",
        "    # print(np.arange(self.max_sequence_length))\n",
        "    # print(inputs.shape[0])\n",
        "\n",
        "    # position = np.array([np.arange(self.max_sequence_length)[:, np.newaxis] for _ in range(batch_size)])\n",
        "    bs  = tf.shape(inputs)[0]\n",
        "    # print(np.arange(self.max_sequence_length))\n",
        "    # print(inputs.shape[0])\n",
        "    # 需要將position 改為使用Tensor做成\n",
        "\n",
        "    # position = np.array([np.arange(self.max_sequence_length)[:, np.newaxis] for _ in range(batch_size)])\n",
        "    # 定義 while_loop 的條件函數\n",
        "    positions = tf.zeros([max_sequence_length, 0, 1], dtype=tf.float32)\n",
        "    def condition(i, positions):\n",
        "        return i < bs\n",
        "\n",
        "    # 定義 while_loop 的主體函數\n",
        "    def body(i, positions):\n",
        "        # 在這裡進行每個迴圈的操作\n",
        "        r = tf.range(max_sequence_length, dtype=tf.float32)\n",
        "        r = tf.expand_dims(r, axis=-1)\n",
        "        positions = tf.concat([positions, tf.expand_dims(r, axis=1)], axis=1)\n",
        "        return i + 1, positions\n",
        "\n",
        "    # 使用 tf.while_loop 進行迴圈\n",
        "    i = tf.constant(0)\n",
        "    _, positions = tf.while_loop(condition, body, [i, positions],shape_invariants=[i.get_shape(), tf.TensorShape([None, None, 1])])\n",
        "    # 轉換形狀\n",
        "    positions = tf.reshape(positions, [bs, max_sequence_length, 1])\n",
        "    # print('position size:',position.shape)\n",
        "    d_model = inputs.shape[-1]\n",
        "    # print('d_model:',d_model)\n",
        "    time_embedding = get_temporal_embeddings(positions, d_model)\n",
        "    # print('time_embedding size:',time_embedding.shape)\n",
        "    inputs = inputs[:,tf.newaxis,:]\n",
        "    tf.TensorShape([time_embedding.shape[0]]).concatenate(tf.TensorShape(inputs.shape[1:]))\n",
        "    # print(inputs.shape)\n",
        "    # print(time_embedding.shape)\n",
        "    combined_embedding = inputs + time_embedding\n",
        "    # print('combined_embedding size:',combined_embedding.shape)\n",
        "    return combined_embedding * self.temporal_weights # n*max_seq*d_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWpRbh0CPIIM"
      },
      "source": [
        "### Multihead Attention 層\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zOi9pFm3e_qL"
      },
      "outputs": [],
      "source": [
        "def scaled_dot_product(q,k,v,mask=None):\n",
        "    # q,k,b = 30 x 8 x 128 x 64\n",
        "    d_k = q.shape[-1] # 64\n",
        "    # 計算scaled\n",
        "    num_dimensions = tf.rank(k)# 獲取 tensor 的維度數\n",
        "    perm = tf.concat([tf.range(num_dimensions - 2), tf.range(num_dimensions - 1, num_dimensions - 3, -1)], axis=0)\n",
        "    scaled = tf.matmul(q,tf.transpose(k,perm=perm))/math.sqrt(d_k) # 30 x 8 x 128 x 128\n",
        "\n",
        "    if mask:\n",
        "        # masking for decoder\n",
        "        mask_ = tf.fill(scaled.shape,float('-inf'))\n",
        "        mask_ = tf.experimental.numpy.triu(mask_,k=1) # k=1 對角線右邊一個\n",
        "        scaled += mask_ # 30 x 8 x 128 x 128\n",
        "    attention = tf.nn.softmax(scaled,axis=-1) # 30 x 8 x 128 x 128\n",
        "    values = tf.matmul(attention,v) # 30 x 8 x 200 x 64\n",
        "    return values,attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAPQK-JZO2h8"
      },
      "outputs": [],
      "source": [
        "\n",
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,d_model,num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model # 512\n",
        "    self.num_heads = num_heads # 8\n",
        "    self.head_dim = d_model // num_heads # e.g. 512 // 8 = 64\n",
        "    self.qkv_layer = Dense(3*d_model,input_shape=(d_model,)) # 512 x 1536\n",
        "    self.linear_layer = Dense(d_model,input_shape=(d_model,)) # 512 x 512\n",
        "    # self.batch_size = batch_size\n",
        "    # 設定trainable\n",
        "    self.trainable = False\n",
        "\n",
        "  def call(self,x,mask=None): # mask 另外弄\n",
        "\n",
        "    _,sequence_length,d_model = x.shape # 30 x 128 x 512\n",
        "    batch_size = tf.shape(x)[0]\n",
        "    # print(f\"x shape: {x.shape}\")\n",
        "\n",
        "    qkv = self.qkv_layer(x) # 30 x 128 x 1536\n",
        "    # print(f\"qkv shape: {qkv.shape}\")\n",
        "\n",
        "    qkv = tf.reshape(qkv,[batch_size,sequence_length,self.num_heads,3*self.head_dim]) # 30 x 128 x 8 x 192(64*3)\n",
        "    # print(f\"qkv shape: {qkv.shape}\")\n",
        "    qkv = tf.transpose(qkv,perm=[0,2,1,3]) # 30 x 8 x 128 x 192\n",
        "    # print(f\"qkv shape: {qkv.shape}\")\n",
        "    q,k,v, = tf.split(qkv,3,axis=-1) # each are 30 x 8 x 128 x 64\n",
        "    # print(f\"q shape: {q.shape} || k shape: {k.shape} || v shape: {v.shape}\")\n",
        "    values,attention = scaled_dot_product(q,k,v,mask) # attention = 30 x 8 x 128 x 128, values = 30 x 8 x 128 x 64\n",
        "#         print(f\"values shape: {values.shape} || attention shape: {attention.shape}\")\n",
        "    values = tf.reshape(values,[batch_size,sequence_length,self.num_heads*self.head_dim]) # 30 x 128 x 512\n",
        "#         print(f\"values shape: {values.shape}\")\n",
        "    out = self.linear_layer(values)\n",
        "#         print(f\"out shape: {out.shape}\")\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDlpf6ozPMaQ"
      },
      "outputs": [],
      "source": [
        "class MultiHeadCrossAttention(tf.keras.layers.Layer):\n",
        "  def __init__(self,d_model,num_heads):\n",
        "    super().__init__()\n",
        "    self.d_model = d_model # 512\n",
        "    self.num_heads = num_heads # 8\n",
        "    self.head_dim = d_model // num_heads # e.g. 512 // 8 = 64\n",
        "    self.kv_layer = Dense(2*d_model,input_shape=(d_model,)) # 512 x 1024\n",
        "    self.q_layer = Dense(d_model,input_shape=(d_model,)) # 512 x 512\n",
        "    self.linear_layer = Dense(d_model,input_shape=(d_model,)) # 512 x 512\n",
        "    # 設定trainable\n",
        "    self.trainable = False\n",
        "    # self.batch_size = batch_size\n",
        "\n",
        "  def call(self,x,y,mask=None): # mask 另外弄\n",
        "\n",
        "    _,sequence_length,d_model = x.shape # 30 x 128 x 512\n",
        "    batch_size = tf.shape(x)[0]\n",
        "#         print(f\"x shape: {x.shape}\")\n",
        "    kv = self.kv_layer(x) # 30 x 128 x 1024\n",
        "#         print(f\"qkv shape: {qkv.shape}\")\n",
        "    q =  self.q_layer(y) # 30 x 128 x 512\n",
        "    kv = tf.reshape(kv,[batch_size,sequence_length,self.num_heads,2*self.head_dim]) # 30 x 128 x 8 x 128\n",
        "    q = tf.reshape(q,[batch_size,sequence_length,self.num_heads,self.head_dim]) # 30 x 128 x 8 x 64\n",
        "#         print(f\"qkv shape: {qkv.shape}\")\n",
        "    kv = tf.transpose(kv,perm=[0,2,1,3]) # 30 x 8 x 128 x 128\n",
        "    q = tf.transpose(q,perm=[0,2,1,3]) # 30 x 8 x 128 x 64\n",
        "#         print(f\"qkv shape: {qkv.shape}\")\n",
        "    k,v = tf.split(kv,2,axis=-1) # k: 30 x 8 x 128 x 64 v: 30 x 8 x 128 x 64\n",
        "#         print(f\"q shape: {q.shape} || k shape: {k.shape} || v shape: {v.shape}\")\n",
        "    values,attention = scaled_dot_product(q,k,v,mask) # attention = 30 x 8 x 128 x 128, values = 30 x 8 x 128 x 64\n",
        "#         print(f\"values shape: {values.shape} || attention shape: {attention.shape}\")\n",
        "    values = tf.reshape(values,[batch_size,sequence_length,d_model]) # 30 x 128 x 512\n",
        "#         print(f\"values shape: {values.shape}\")\n",
        "    out = self.linear_layer(values) # 30 x 128 x 512\n",
        "#         print(f\"out shape: {out.shape}\")\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fqf0CyG7PQVq"
      },
      "source": [
        "### Layer Normalization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWzFJoM6PYMD"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(tf.keras.layers.Layer):\n",
        "  def __init__(self,parameter_shape,eps=1e-5):\n",
        "    super().__init__()\n",
        "    self.parameter_shape = parameter_shape # [512]\n",
        "    self.eps = eps\n",
        "    self.gamma = tf.Variable(tf.ones(parameter_shape)) # [512]\n",
        "    self.beta = tf.Variable(tf.zeros(parameter_shape)) # [512]\n",
        "\n",
        "  def call(self,x):\n",
        "    # input = 30 x 200 x 512\n",
        "    dims = [-(i+1) for i in range(len(self.parameter_shape))] # [-1]\n",
        "    mean = tf.reduce_mean(x,axis=dims,keepdims=True) # 30 x 128 x 1\n",
        "#         print(f\"Mean \\n ({mean.shape}): \\n {mean}\")\n",
        "    var = tf.reduce_mean(((x-mean)**2),axis=dims,keepdims=True) # 30 x 128 x 1\n",
        "    std = tf.math.sqrt(var+self.eps) # 30 x 128 x 1\n",
        "#         print(f\"Standard Deviation \\n ({std.shape}): \\n {std}\")\n",
        "    y = (x - mean) / std # 30 x 128 x 512\n",
        "#         print(f\"y \\n ({y.shape}) = \\n {y}\")\n",
        "    out = self.gamma * y + self.beta # # 30 x 128 x 512\n",
        "#         print(f\"out \\n ({out.shape}) = \\n {out}\")\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FmTAl0HHP8R-"
      },
      "source": [
        "### Feed Forward (Fully Connected Network)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c9zXmcLiQDMI"
      },
      "outputs": [],
      "source": [
        "class PositionwiseFeedForward(tf.keras.layers.Layer):\n",
        "  def __init__(self,d_model,hidden,drop_prob=0.1):\n",
        "\n",
        "    super(PositionwiseFeedForward,self).__init__()\n",
        "    self.linear1 = Dense(hidden,input_shape=(d_model,)) # 512 x 2048\n",
        "    self.linear2 = Dense(d_model,input_shape=(hidden,)) # 2048 x 512\n",
        "    self.relu = tf.keras.layers.ReLU() #\n",
        "    self.dropout = Dropout(drop_prob)\n",
        "    # 設定trainable\n",
        "    self.trainable = False\n",
        "\n",
        "  def call(self,x):\n",
        "    # input = 30 x 128 x 512\n",
        "    x = self.linear1(x) # 30 x 128 x 2048\n",
        "  #         print(f\"x \\n {x.shape}\")\n",
        "    x = self.relu(x) # 30 x 128 x 2048\n",
        "  #         print(f\"x \\n {x.shape}\")\n",
        "    x = self.dropout(x) # 30 x 128 x 2048\n",
        "  #         print(f\"x \\n {x.shape}\")\n",
        "    x = self.linear2(x) # 30 x 128 x 512\n",
        "  #         print(f\"x \\n {x.shape}\")\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KAwnTAR8RMt8"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IV1zF92ERXaE"
      },
      "source": [
        "### Encoder Block(Layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzh4_t0qRa9T"
      },
      "outputs": [],
      "source": [
        "class EncoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob):\n",
        "\n",
        "    super(EncoderLayer,self).__init__()\n",
        "    self.attention = MultiHeadAttention(d_model=d_model,num_heads=num_heads)\n",
        "    self.norm1 = LayerNorm(parameter_shape=[d_model])\n",
        "    self.dropout1 = Dropout(drop_prob)\n",
        "    self.ffn = PositionwiseFeedForward(d_model=d_model,hidden=ffn_hidden,drop_prob=drop_prob )\n",
        "    self.norm2 = LayerNorm(parameter_shape=[d_model])\n",
        "    self.dropout2 = Dropout(drop_prob)\n",
        "    # 設定trainable\n",
        "    self.trainable = False\n",
        "  def call(self,x,mask):\n",
        "    residual_x = x # 30 x 128 x 512\n",
        "    x = self.attention(x,mask=mask) # 30 x 128 x 512\n",
        "    x = self.dropout1(x) # 30 x 128 x 512\n",
        "    x = self.norm1(x+residual_x) # 30 x 128 x 512\n",
        "    residual_x = x # 30 x 128 x 512\n",
        "    x = self.ffn(x) # 30 x 128 x 512\n",
        "    x = self.dropout2(x) # 30 x 128 x 512\n",
        "    x= self.norm2(x+residual_x) # 30 x 128 x 512\n",
        "\n",
        "    return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNkDjAdZRQTU"
      },
      "outputs": [],
      "source": [
        "# Encoder\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob,num_layers,\n",
        "                max_sequence_length,lan_to_index=None,START_TOKEN=None,END_TOKEN=None,PADDING_TOKEN=None):\n",
        "    super().__init__()\n",
        "    self.sequence_embedding = SequenceEmbedding(max_sequence_length,d_model,lan_to_index,START_TOKEN,END_TOKEN,PADDING_TOKEN)\n",
        "    self.layers = Sequential()\n",
        "    for _ in range(num_layers):\n",
        "        self.layers.add(EncoderLayer(d_model,ffn_hidden,num_heads,drop_prob))\n",
        "\n",
        "    # 設定trainable\n",
        "    self.trainable = False\n",
        "\n",
        "  def call(self,x,mask,batch_size):\n",
        "    x = self.sequence_embedding(x,batch_size)\n",
        "    # print(x)\n",
        "    x = self.layers(x,mask)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLsrTayvTCc8"
      },
      "source": [
        "## Decoder\n",
        "由於GAN中只關注身為Generator的Transformer的產出，不需要使用到Decoder的部分，所以在Transformer中也沒有加入Decoder，但為了完整起見，還是將code呈現上來"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvMrb71KTrjN"
      },
      "source": [
        "### Decoder Block(Layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sa6wBog9Tqo2"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob):\n",
        "\n",
        "    super(DecoderLayer,self).__init__()\n",
        "    self.self_attention = MultiHeadAttention(d_model=d_model,num_heads=num_heads)\n",
        "    self.norm1 = LayerNorm(parameter_shape=[d_model])\n",
        "    self.dropout1 = Dropout(drop_prob)\n",
        "    # cross attention here\n",
        "    self.encoder_decoder_attention =  MultiHeadCrossAttention(d_model=d_model,num_heads=num_heads)\n",
        "    self.norm2 = LayerNorm(parameter_shape=[d_model])\n",
        "    self.dropout2 = Dropout(drop_prob)\n",
        "    self.ffn = PositionwiseFeedForward(d_model=d_model,hidden=ffn_hidden,drop_prob=drop_prob)\n",
        "    self.norm3 = LayerNorm(parameter_shape=[d_model])\n",
        "    self.dropout3 = Dropout(drop_prob)\n",
        "    # 設定trainable\n",
        "    self.trainable = False\n",
        "\n",
        "  def call(self,x,y,mask=None,cross_mask=None):\n",
        "        # print('y as input of layer',y)\n",
        "    _y = y # for residual # 30 x 128 x 512\n",
        "    y = self.self_attention(y,mask=mask) # 30 x 128 x 512\n",
        "    y = self.dropout1(y) # 30 x 128 x 512\n",
        "    y = self.norm1(y+_y) # 30 x 128 x 512\n",
        "\n",
        "    _y = y # for residual # 30 x 128 x 512\n",
        "    y = self.ffn(y)\n",
        "    # Cross attention\n",
        "    y = self.encoder_decoder_attention(x,y,mask=cross_mask)\n",
        "    y = self.dropout2(y)\n",
        "    y = self.norm2(y+_y)\n",
        "    # print('y in layers',y)\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aKcgnFrCT8GB"
      },
      "outputs": [],
      "source": [
        "# 為了自定義input (X,Y) 所以需要自訂Seuqential\n",
        "class SequentialDecoder(Sequential):\n",
        "  def call(self,*inputs):\n",
        "    x,y,mask,cross_mask = inputs\n",
        "    for layer in self.layers:\n",
        "        y = layer(x,y,mask,cross_mask) # 30 x 128 x 512\n",
        "\n",
        "    return y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YeEfHkBJRJEM"
      },
      "outputs": [],
      "source": [
        "# Decoder\n",
        "class Decoder(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob,num_layers,\n",
        "              max_sequence_length,lan_to_index=None,START_TOKEN=None,END_TOKEN=None,PADDING_TOKEN=None):\n",
        "\n",
        "    super().__init__()\n",
        "    self.sequence_embedding = SequenceEmbedding(max_sequence_length,d_model,lan_to_index,START_TOKEN,END_TOKEN,PADDING_TOKEN)\n",
        "    self.layers  = SequentialDecoder()\n",
        "    for _ in range(num_layers):\n",
        "      self.layers.add(DecoderLayer(d_model,ffn_hidden,num_heads,drop_prob))\n",
        "    # 設定trainable\n",
        "    self.trainable = False\n",
        "\n",
        "\n",
        "  def call(self,x,y,mask,cross_mask,AT_table,batch_):\n",
        "    # AT_table is for autoregressive loop 的時候 做tensor_scatter_nd_add() 的indices 使用的\n",
        "    # x: 30 x 128 x 512\n",
        "    # y: 30 x 128 x 512cab_size\n",
        "    # mask: 128 x 128\n",
        "    batch_size = tf.shape(y)[0]\n",
        "    y = self.sequence_embedding(y,batch_)\n",
        "\n",
        "    num = int(y.shape[1])\n",
        "    for i in range(num):\n",
        "\n",
        "      # print('y delta:',y[:,:i+1,:])\n",
        "      att_output = self.layers(x[:,:i+1,:],y[:,:i+1,:],mask,cross_mask)\n",
        "      # print('att_output:',att_output)\n",
        "      att_output = tf.reshape(att_output,[-1])\n",
        "      # print('y',y)\n",
        "\n",
        "      # 因為有特別製作indx_table, 這樣就不用在網路反覆運算一樣且可以重複使用的東西\n",
        "      # temp = tf.constant(indx_table[str(i)])[:,:2]\n",
        "      # indices 也要 跟著y[:,:i+1,:]增加\n",
        "      indices = AT_table[:i+1,:,:]\n",
        "      indices = tf.reshape(indices,[indices.shape[0]*indices.shape[1],indices.shape[2]])\n",
        "      # print('now in loop:',i)\n",
        "      # print('scatter y:',y)\n",
        "      # print('scatter indices:',indices)\n",
        "      # print('scatter att_output:',att_output)\n",
        "      y = tf.tensor_scatter_nd_add(y,indices,att_output)\n",
        "      # indices = tf.range(tf.shape[y][1])\n",
        "      # y[:,i,:] += att_output[:,-1,:]\n",
        "\n",
        "    # print(y)\n",
        "    return y# 30 x 128 x 512"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bv3si7UDUa6i"
      },
      "source": [
        "## Transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7v_jKI45UpMt"
      },
      "outputs": [],
      "source": [
        "# 沒有 Decoder\n",
        "class TransformerEncoderOnly(Model):\n",
        "\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob,num_layers,\n",
        "              max_sequence_length,vocab_size,origin_to_index=None,transform_to_index=None,\n",
        "                START_TOKEN=None,END_TOKEN=None,PADDING_TOKEN=None):\n",
        "\n",
        "    super().__init__()\n",
        "    self.start = START_TOKEN\n",
        "    self.padding = PADDING_TOKEN\n",
        "    self.end = END_TOKEN\n",
        "    self.transform_to_indx = transform_to_index\n",
        "    self.vocab_size = vocab_size\n",
        "    self.max_seq_len = max_sequence_length\n",
        "    self.input_layer = Input(shape=(BATCH,max_sequence_length))\n",
        "    self.encoder = Encoder(d_model,ffn_hidden,num_heads,drop_prob,num_layers,max_sequence_length,origin_to_index,START_TOKEN,END_TOKEN,PADDING_TOKEN)\n",
        "    self.decoder = Decoder(d_model,ffn_hidden,num_heads,drop_prob,num_layers,max_sequence_length,transform_to_index,START_TOKEN,END_TOKEN,PADDING_TOKEN)\n",
        "    self.linear = Dense(vocab_size,input_shape=(d_model,))\n",
        "    # 設定trainable\n",
        "    self.trainable = False\n",
        "\n",
        "  def call(self,inputs,batch_=1,encoder_mask=None,decoder_mask=None,cross_mask=None,): # x, y are batch of sentence\n",
        "\n",
        "    # 準備好輸出\n",
        "    output = []\n",
        "\n",
        "    # 生成一個空的值(with start)\n",
        "    x = inputs\n",
        "\n",
        "    batch_size = tf.shape(x)[0]\n",
        "\n",
        "\n",
        "    print(x.shape)\n",
        "    x = tf.reshape(x,[batch_size,x.shape[1]])\n",
        "    x = tf.cast(x,dtype=tf.float32)\n",
        "\n",
        "    out = self.encoder(x,encoder_mask,batch_)\n",
        "\n",
        "    # print('decoder output',out)\n",
        "    out = self.linear(out)\n",
        "    # print('linear',out)\n",
        "    out = tf.argmax(out,axis=2)\n",
        "    # print('output',out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJlqgEQRUTUp"
      },
      "outputs": [],
      "source": [
        "# 有 Decoder\n",
        "class Transformer(Model):\n",
        "\n",
        "  def __init__(self,d_model,ffn_hidden,num_heads,drop_prob,num_layers,\n",
        "              max_sequence_length,vocab_size,origin_to_index,transform_to_index,\n",
        "                START_TOKEN,END_TOKEN,PADDING_TOKEN):\n",
        "\n",
        "    super().__init__()\n",
        "    self.start = START_TOKEN\n",
        "    self.padding = PADDING_TOKEN\n",
        "    self.end = END_TOKEN\n",
        "    self.transform_to_indx = transform_to_index\n",
        "    self.vocab_size = vocab_size\n",
        "    self.max_seq_len = max_sequence_length\n",
        "    self.encoder = Encoder(d_model,ffn_hidden,num_heads,drop_prob,num_layers,max_sequence_length,origin_to_index,START_TOKEN,END_TOKEN,PADDING_TOKEN)\n",
        "    self.decoder = Decoder(d_model,ffn_hidden,num_heads,drop_prob,num_layers,max_sequence_length,transform_to_index,START_TOKEN,END_TOKEN,PADDING_TOKEN)\n",
        "    self.linear = Dense(vocab_size,input_shape=(d_model,))\n",
        "    # 設定trainable\n",
        "    self.trainable = False\n",
        "\n",
        "  def call(self,inputs,y,AT_table,batch_,encoder_mask=None,decoder_mask=None,cross_mask=None,): # x, y are batch of sentence\n",
        "\n",
        "    # 準備好輸出\n",
        "    output = []\n",
        "\n",
        "    # 生成一個空的值(with start)\n",
        "    # y = [random.randint(0,131) for _ in range(self.max_seq_len)]\n",
        "\n",
        "    x = inputs\n",
        "\n",
        "    batch_size = tf.shape(x)[0]\n",
        "    # print(batch_size)\n",
        "    # print(y_)\n",
        "    # y_ = tf.constant(y_,dtype=tf.float32)\n",
        "\n",
        "\n",
        "    y = tf.reshape(y,[batch_size,y.shape[1]])\n",
        "    y = tf.cast(y,dtype=tf.float32)\n",
        "    # print(y)\n",
        "\n",
        "    # print('y shape at Decoder input',y.shape)\n",
        "    # y = tf.one_hot(y,self.vocab_size)\n",
        "    # y = tf.reshape(y,[1,y.shape[0],y.shape[1]])\n",
        "    # print('X for encoder:',x)\n",
        "    x = self.encoder(x,encoder_mask,batch_)\n",
        "\n",
        "\n",
        "    # print(self.max_seq_len,'/',self.delta**(-1))\n",
        "\n",
        "\n",
        "    out = self.decoder(x,y,decoder_mask,cross_mask,AT_table,batch_)\n",
        "    # print('decoder output',out)\n",
        "    out = self.linear(out)\n",
        "    # print('linear',out)\n",
        "    out = tf.argmax(out,axis=2)\n",
        "    # print('output',out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MU1agpBqc_3o"
      },
      "source": [
        "# Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z_IhSNMDnSHM"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import InputLayer,LSTM,Dropout,LeakyReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zClRcO2PVB1Y"
      },
      "outputs": [],
      "source": [
        "class Discriminator(Model):\n",
        "\n",
        "  def __init__(self,input_shape):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.model = tf.keras.Sequential([\n",
        "        InputLayer(input_shape=input_shape),\n",
        "        LSTM(256,input_shape=input_shape),\n",
        "        Dropout(0.2),\n",
        "        LeakyReLU(0.2),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "  def call(self, inputs, training):\n",
        "    return self.model(inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydcS-EwadD9u"
      },
      "source": [
        "# GAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gemgwtYcVevq"
      },
      "outputs": [],
      "source": [
        "clip_value = 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F7e03iVXVKUc"
      },
      "outputs": [],
      "source": [
        "class TransformerGAN(Model):\n",
        "\n",
        "  def __init__(self,generator,discriminator,*args,**kwargs):\n",
        "    # Pass through arg and kwargs to base class\n",
        "    super().__init__(*args,**kwargs)\n",
        "\n",
        "    # Create attribute for gen and disc\n",
        "    self.generator = generator\n",
        "    self.discriminator = discriminator\n",
        "    self.table  = indices_dict(BATCH)\n",
        "  def compile(self,g_opt,d_opt,g_loss,d_loss,*args,**kwargs):\n",
        "    # Compile with base class\n",
        "    super().compile(*args,**kwargs)\n",
        "\n",
        "    # Create attribute for Losses and optimizers\n",
        "    self.g_opt = g_opt\n",
        "    self.d_opt = d_opt\n",
        "    self.g_loss = g_loss\n",
        "    self.d_loss = d_loss\n",
        "    self.g_loss_series = []\n",
        "    self.d_loss_series = []\n",
        "\n",
        "  def train_step(self,data):\n",
        "    batch,label = data\n",
        "    batch = tf.cast(batch,dtype=tf.float32)\n",
        "\n",
        "    batch_size = tf.shape(batch)[0]\n",
        "    noise = tf.random.normal(shape=(batch_size,tf.shape(batch)[1]))\n",
        "\n",
        "    with tf.GradientTape() as d_tape:\n",
        "\n",
        "      # print('batch:',batch)\n",
        "      # print('noise',noise)\n",
        "\n",
        "      # generated_music = self.generator(batch,noise,self.table,BATCH,training=False)\n",
        "      # encoder only\n",
        "      # noise 跟 batch 結合\n",
        "\n",
        "      inputs = batch + noise\n",
        "      generated_music = self.generator(inputs,BATCH,training=False)\n",
        "      # print(generated_music)\n",
        "      # print(generated_music.shape)\n",
        "      # 符合LSTM 的dim\n",
        "      y_real = tf.reshape(label,[batch_size,1,label.shape[1]])\n",
        "      generated_music = tf.reshape(generated_music,[batch_size,1,batch.shape[1]])\n",
        "\n",
        "      yhat_real = self.discriminator(y_real,training=True)\n",
        "      yhat_fake = self.discriminator(generated_music,training=True)\n",
        "\n",
        "\n",
        "      # predict\n",
        "      # 比對時原始資料跟generator 生成的資料的shape不一致\n",
        "      yhat_realfake = tf.concat([yhat_real,yhat_fake],axis=0)\n",
        "      # Create labels real and fake images\n",
        "      # actual label\n",
        "      y_realfake = tf.concat([tf.zeros_like(yhat_real),tf.ones_like(yhat_fake)],axis=0)\n",
        "      # Add some noise to the TRUE output\n",
        "      noise_real = 0.5*tf.random.uniform(tf.shape(yhat_real))\n",
        "      noise_fake = -0.5*tf.random.uniform(tf.shape(yhat_fake))\n",
        "      y_realfake += tf.concat([noise_real,noise_fake],axis=0)\n",
        "\n",
        "      total_d_loss = self.d_loss(y_realfake,yhat_realfake)\n",
        "\n",
        "      # Apply backpropagation - nn learn\n",
        "      dgrad = d_tape.gradient(total_d_loss,self.discriminator.trainable_variables)\n",
        "\n",
        "    self.d_opt.apply_gradients(zip(dgrad,self.discriminator.trainable_variables))\n",
        "\n",
        "    with tf.GradientTape() as g_tape:\n",
        "      # Generate some new\n",
        "      tf.random.set_seed(100)\n",
        "\n",
        "      noise = tf.random.normal(shape=(tf.shape(batch)[0],tf.shape(batch)[1]))\n",
        "      # print('noise for gen training:',noise)\n",
        "      # gen_music = self.generator(batch,noise,self.table,BATCH,training=True)\n",
        "      # encoder only\n",
        "      # noise 跟 batch結合\n",
        "      inputs = batch + noise\n",
        "      gen_music = self.generator(inputs,BATCH,training=True)\n",
        "      gen_music = tf.reshape(gen_music,[batch_size,1,gen_music.shape[1]])\n",
        "      # Create the predicted labels\n",
        "      predicted_labels = self.discriminator(gen_music,training=False)\n",
        "      # Calculate loss - trick to training to fake out the discriminator\n",
        "      total_g_loss = self.g_loss(tf.zeros_like(predicted_labels),predicted_labels)\n",
        "      ggrad = g_tape.gradient(total_g_loss,self.generator.trainable_variables)\n",
        "\n",
        "      # 應用梯度之前進行梯度裁剪\n",
        "      ggrad = tf.clip_by_global_norm(ggrad, clip_value)\n",
        "    self.g_opt.apply_gradients(zip(ggrad,self.generator.trainable_variables))\n",
        "\n",
        "    with tf.GradientTape() as g_tape:\n",
        "      # Generate some new\n",
        "      tf.random.set_seed(100)\n",
        "\n",
        "      noise = tf.random.normal(shape=(tf.shape(batch)[0],tf.shape(batch)[1]))\n",
        "      # print('noise for gen training:',noise)\n",
        "      # gen_music = self.generator(batch,noise,self.table,BATCH,training=True)\n",
        "      # encoder only\n",
        "      # noise 跟 batch結合\n",
        "      inputs = batch + noise\n",
        "      gen_music = self.generator(inputs,BATCH,training=True)\n",
        "      gen_music = tf.reshape(gen_music,[batch_size,1,gen_music.shape[1]])\n",
        "      # Create the predicted labels\n",
        "      predicted_labels = self.discriminator(gen_music,training=False)\n",
        "      # Calculate loss - trick to training to fake out the discriminator\n",
        "      total_g_loss = self.g_loss(tf.zeros_like(predicted_labels),predicted_labels)\n",
        "      ggrad = g_tape.gradient(total_g_loss,self.generator.trainable_variables)\n",
        "\n",
        "    self.g_opt.apply_gradients(zip(ggrad,self.generator.trainable_variables))\n",
        "\n",
        "    return {\"d_loss\": total_d_loss,\"g_loss\": total_g_loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9FLBkP02dKE5"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5Gp0m3HWzbr"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.losses import BinaryCrossentropy,MeanSquaredError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W4jbt_0AXeE2"
      },
      "outputs": [],
      "source": [
        "epochs_ = 300\n",
        "batchSize = 4\n",
        "# 弄一個梯度裁剪避免梯度爆炸\n",
        "# 梯度裁剪閥值\n",
        "clip_value = 1.0\n",
        "# clipvalue=clip_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UzIECGoTXZbT"
      },
      "outputs": [],
      "source": [
        "generator = TransformerEncoderOnly(d_model,ffn_hidden,num_heads,drop_prob,num_layers,max_sequence_length,\n",
        "                        vocab_size)\n",
        "discriminator = Discriminator(input_shape=(1,max_sequence_length))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v2WGZlYjVm5J"
      },
      "outputs": [],
      "source": [
        "g_opt = Adam(learning_rate=1e-4)\n",
        "d_opt = Adam(learning_rate=1e-4)\n",
        "# g_loss = BinaryCrossentropy()\n",
        "g_loss = MeanSquaredError()\n",
        "d_loss = BinaryCrossentropy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "grNlHY4qX4BG"
      },
      "outputs": [],
      "source": [
        "gan = TransformerGAN(generator,discriminator)\n",
        "gan.compile(g_opt,d_opt,g_loss,d_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ePcV1kGSX7cQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11dc5064-b8fe-4a25-a3c7-0be3c185a3e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "(None, 128)\n",
            "(None, 128)\n",
            "(None, 128)\n",
            "(None, 128)\n",
            "(None, 128)\n",
            "(None, 128)\n",
            "10938/10938 [==============================] - 215s 18ms/step - d_loss: 0.5665 - g_loss: 0.3703\n",
            "Epoch 2/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5652 - g_loss: 0.3899\n",
            "Epoch 3/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5648 - g_loss: 0.3983\n",
            "Epoch 4/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5650 - g_loss: 0.3994\n",
            "Epoch 5/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5655 - g_loss: 0.4133\n",
            "Epoch 6/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5658 - g_loss: 0.4078\n",
            "Epoch 7/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5656 - g_loss: 0.4090\n",
            "Epoch 8/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5658 - g_loss: 0.4172\n",
            "Epoch 9/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5655 - g_loss: 0.4275\n",
            "Epoch 10/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5655 - g_loss: 0.4274\n",
            "Epoch 11/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5660 - g_loss: 0.4188\n",
            "Epoch 12/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5660 - g_loss: 0.4312\n",
            "Epoch 13/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5660 - g_loss: 0.4281\n",
            "Epoch 14/300\n",
            "10938/10938 [==============================] - 205s 19ms/step - d_loss: 0.5664 - g_loss: 0.4505\n",
            "Epoch 15/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5667 - g_loss: 0.4355\n",
            "Epoch 16/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5667 - g_loss: 0.4421\n",
            "Epoch 17/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5672 - g_loss: 0.4274\n",
            "Epoch 18/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5674 - g_loss: 0.4437\n",
            "Epoch 19/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5671 - g_loss: 0.4352\n",
            "Epoch 20/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5674 - g_loss: 0.4500\n",
            "Epoch 21/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5671 - g_loss: 0.4391\n",
            "Epoch 22/300\n",
            "10938/10938 [==============================] - 202s 18ms/step - d_loss: 0.5663 - g_loss: 0.4415\n",
            "Epoch 23/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5675 - g_loss: 0.4439\n",
            "Epoch 24/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5672 - g_loss: 0.4527\n",
            "Epoch 25/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5675 - g_loss: 0.4594\n",
            "Epoch 26/300\n",
            "10938/10938 [==============================] - 202s 18ms/step - d_loss: 0.5684 - g_loss: 0.4466\n",
            "Epoch 27/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5677 - g_loss: 0.4501\n",
            "Epoch 28/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5680 - g_loss: 0.4505\n",
            "Epoch 29/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5683 - g_loss: 0.4392\n",
            "Epoch 30/300\n",
            "10938/10938 [==============================] - 203s 19ms/step - d_loss: 0.5679 - g_loss: 0.4543\n",
            "Epoch 31/300\n",
            "10938/10938 [==============================] - 205s 19ms/step - d_loss: 0.5685 - g_loss: 0.4227\n",
            "Epoch 32/300\n",
            "10938/10938 [==============================] - 218s 20ms/step - d_loss: 0.5681 - g_loss: 0.4552\n",
            "Epoch 33/300\n",
            "10938/10938 [==============================] - 218s 20ms/step - d_loss: 0.5679 - g_loss: 0.4587\n",
            "Epoch 34/300\n",
            "10938/10938 [==============================] - 215s 20ms/step - d_loss: 0.5690 - g_loss: 0.4418\n",
            "Epoch 35/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5680 - g_loss: 0.4722\n",
            "Epoch 36/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5680 - g_loss: 0.4588\n",
            "Epoch 37/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5685 - g_loss: 0.4405\n",
            "Epoch 38/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5690 - g_loss: 0.4739\n",
            "Epoch 39/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5687 - g_loss: 0.4629\n",
            "Epoch 40/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5689 - g_loss: 0.4431\n",
            "Epoch 41/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5688 - g_loss: 0.4454\n",
            "Epoch 42/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5687 - g_loss: 0.4671\n",
            "Epoch 43/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5679 - g_loss: 0.4571\n",
            "Epoch 44/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5682 - g_loss: 0.4569\n",
            "Epoch 45/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5685 - g_loss: 0.4552\n",
            "Epoch 46/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5680 - g_loss: 0.4512\n",
            "Epoch 47/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5680 - g_loss: 0.4300\n",
            "Epoch 48/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5689 - g_loss: 0.4485\n",
            "Epoch 49/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5681 - g_loss: 0.4522\n",
            "Epoch 50/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5680 - g_loss: 0.4713\n",
            "Epoch 51/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5685 - g_loss: 0.4654\n",
            "Epoch 52/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5682 - g_loss: 0.4484\n",
            "Epoch 53/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5685 - g_loss: 0.4629\n",
            "Epoch 54/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5688 - g_loss: 0.4647\n",
            "Epoch 55/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5684 - g_loss: 0.4618\n",
            "Epoch 56/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5686 - g_loss: 0.4628\n",
            "Epoch 57/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5682 - g_loss: 0.4568\n",
            "Epoch 58/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5681 - g_loss: 0.4583\n",
            "Epoch 59/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5684 - g_loss: 0.4622\n",
            "Epoch 60/300\n",
            "10938/10938 [==============================] - 202s 18ms/step - d_loss: 0.5688 - g_loss: 0.4420\n",
            "Epoch 61/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5684 - g_loss: 0.4493\n",
            "Epoch 62/300\n",
            "10938/10938 [==============================] - 204s 19ms/step - d_loss: 0.5686 - g_loss: 0.4528\n",
            "Epoch 63/300\n",
            "10938/10938 [==============================] - 206s 19ms/step - d_loss: 0.5686 - g_loss: 0.4524\n",
            "Epoch 64/300\n",
            "10938/10938 [==============================] - 204s 19ms/step - d_loss: 0.5688 - g_loss: 0.4669\n",
            "Epoch 65/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5687 - g_loss: 0.4586\n",
            "Epoch 66/300\n",
            "10938/10938 [==============================] - 202s 18ms/step - d_loss: 0.5690 - g_loss: 0.4512\n",
            "Epoch 67/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5682 - g_loss: 0.4808\n",
            "Epoch 68/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5682 - g_loss: 0.4740\n",
            "Epoch 69/300\n",
            "10938/10938 [==============================] - 206s 19ms/step - d_loss: 0.5681 - g_loss: 0.4432\n",
            "Epoch 70/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5677 - g_loss: 0.4669\n",
            "Epoch 71/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5687 - g_loss: 0.4801\n",
            "Epoch 72/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5683 - g_loss: 0.4889\n",
            "Epoch 73/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5682 - g_loss: 0.4713\n",
            "Epoch 74/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5679 - g_loss: 0.4801\n",
            "Epoch 75/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5689 - g_loss: 0.4757\n",
            "Epoch 76/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5683 - g_loss: 0.4545\n",
            "Epoch 77/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5686 - g_loss: 0.4414\n",
            "Epoch 78/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5686 - g_loss: 0.4760\n",
            "Epoch 79/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5681 - g_loss: 0.4510\n",
            "Epoch 80/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5684 - g_loss: 0.4470\n",
            "Epoch 81/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5689 - g_loss: 0.4524\n",
            "Epoch 82/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5684 - g_loss: 0.4455\n",
            "Epoch 83/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5688 - g_loss: 0.4703\n",
            "Epoch 84/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5679 - g_loss: 0.4745\n",
            "Epoch 85/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5681 - g_loss: 0.4421\n",
            "Epoch 86/300\n",
            "10938/10938 [==============================] - 202s 18ms/step - d_loss: 0.5684 - g_loss: 0.4513\n",
            "Epoch 87/300\n",
            "10938/10938 [==============================] - 202s 18ms/step - d_loss: 0.5686 - g_loss: 0.4718\n",
            "Epoch 88/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5682 - g_loss: 0.4604\n",
            "Epoch 89/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5680 - g_loss: 0.4579\n",
            "Epoch 90/300\n",
            "10938/10938 [==============================] - 202s 18ms/step - d_loss: 0.5681 - g_loss: 0.4762\n",
            "Epoch 91/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5684 - g_loss: 0.4846\n",
            "Epoch 92/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5679 - g_loss: 0.4410\n",
            "Epoch 93/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5689 - g_loss: 0.4581\n",
            "Epoch 94/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5681 - g_loss: 0.4424\n",
            "Epoch 95/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5681 - g_loss: 0.4362\n",
            "Epoch 96/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5680 - g_loss: 0.4708\n",
            "Epoch 97/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5677 - g_loss: 0.4593\n",
            "Epoch 98/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5683 - g_loss: 0.4849\n",
            "Epoch 99/300\n",
            "10938/10938 [==============================] - 202s 19ms/step - d_loss: 0.5682 - g_loss: 0.4626\n",
            "Epoch 100/300\n",
            "10938/10938 [==============================] - 203s 19ms/step - d_loss: 0.5683 - g_loss: 0.4296\n",
            "Epoch 101/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5687 - g_loss: 0.4729\n",
            "Epoch 102/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5679 - g_loss: 0.4800\n",
            "Epoch 103/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5678 - g_loss: 0.4701\n",
            "Epoch 104/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5682 - g_loss: 0.4535\n",
            "Epoch 105/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5682 - g_loss: 0.4644\n",
            "Epoch 106/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5685 - g_loss: 0.4554\n",
            "Epoch 107/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5681 - g_loss: 0.4474\n",
            "Epoch 108/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5684 - g_loss: 0.4510\n",
            "Epoch 109/300\n",
            "10938/10938 [==============================] - 198s 18ms/step - d_loss: 0.5689 - g_loss: 0.4924\n",
            "Epoch 110/300\n",
            "10938/10938 [==============================] - 198s 18ms/step - d_loss: 0.5686 - g_loss: 0.4771\n",
            "Epoch 111/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5683 - g_loss: 0.4701\n",
            "Epoch 112/300\n",
            "10938/10938 [==============================] - 198s 18ms/step - d_loss: 0.5686 - g_loss: 0.4688\n",
            "Epoch 113/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5684 - g_loss: 0.4575\n",
            "Epoch 114/300\n",
            "10938/10938 [==============================] - 203s 19ms/step - d_loss: 0.5679 - g_loss: 0.4648\n",
            "Epoch 115/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5677 - g_loss: 0.4523\n",
            "Epoch 116/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5685 - g_loss: 0.4497\n",
            "Epoch 117/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5684 - g_loss: 0.4740\n",
            "Epoch 118/300\n",
            "10938/10938 [==============================] - 198s 18ms/step - d_loss: 0.5679 - g_loss: 0.4718\n",
            "Epoch 119/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5680 - g_loss: 0.4655\n",
            "Epoch 120/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5676 - g_loss: 0.4893\n",
            "Epoch 121/300\n",
            "10938/10938 [==============================] - 203s 19ms/step - d_loss: 0.5678 - g_loss: 0.4602\n",
            "Epoch 122/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5678 - g_loss: 0.4604\n",
            "Epoch 123/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5680 - g_loss: 0.4748\n",
            "Epoch 124/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5690 - g_loss: 0.4726\n",
            "Epoch 125/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5685 - g_loss: 0.4648\n",
            "Epoch 126/300\n",
            "10938/10938 [==============================] - 203s 19ms/step - d_loss: 0.5689 - g_loss: 0.4818\n",
            "Epoch 127/300\n",
            "10938/10938 [==============================] - 204s 19ms/step - d_loss: 0.5685 - g_loss: 0.4625\n",
            "Epoch 128/300\n",
            "10938/10938 [==============================] - 205s 19ms/step - d_loss: 0.5686 - g_loss: 0.4519\n",
            "Epoch 129/300\n",
            "10938/10938 [==============================] - 204s 19ms/step - d_loss: 0.5690 - g_loss: 0.4722\n",
            "Epoch 130/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5681 - g_loss: 0.4408\n",
            "Epoch 131/300\n",
            "10938/10938 [==============================] - 202s 18ms/step - d_loss: 0.5682 - g_loss: 0.4756\n",
            "Epoch 132/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5687 - g_loss: 0.4671\n",
            "Epoch 133/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5682 - g_loss: 0.4626\n",
            "Epoch 134/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5684 - g_loss: 0.4678\n",
            "Epoch 135/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5677 - g_loss: 0.4878\n",
            "Epoch 136/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5683 - g_loss: 0.4844\n",
            "Epoch 137/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5680 - g_loss: 0.4821\n",
            "Epoch 138/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5681 - g_loss: 0.4951\n",
            "Epoch 139/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5688 - g_loss: 0.4836\n",
            "Epoch 140/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5686 - g_loss: 0.4655\n",
            "Epoch 141/300\n",
            "10938/10938 [==============================] - 203s 19ms/step - d_loss: 0.5683 - g_loss: 0.4886\n",
            "Epoch 142/300\n",
            "10938/10938 [==============================] - 204s 19ms/step - d_loss: 0.5688 - g_loss: 0.4496\n",
            "Epoch 143/300\n",
            "10938/10938 [==============================] - 204s 19ms/step - d_loss: 0.5686 - g_loss: 0.4668\n",
            "Epoch 144/300\n",
            "10938/10938 [==============================] - 205s 19ms/step - d_loss: 0.5686 - g_loss: 0.4570\n",
            "Epoch 145/300\n",
            "10938/10938 [==============================] - 204s 19ms/step - d_loss: 0.5677 - g_loss: 0.4509\n",
            "Epoch 146/300\n",
            "10938/10938 [==============================] - 204s 19ms/step - d_loss: 0.5685 - g_loss: 0.4839\n",
            "Epoch 147/300\n",
            "10938/10938 [==============================] - 206s 19ms/step - d_loss: 0.5679 - g_loss: 0.4637\n",
            "Epoch 148/300\n",
            "10938/10938 [==============================] - 206s 19ms/step - d_loss: 0.5685 - g_loss: 0.4580\n",
            "Epoch 149/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5676 - g_loss: 0.4545\n",
            "Epoch 150/300\n",
            "10938/10938 [==============================] - 198s 18ms/step - d_loss: 0.5687 - g_loss: 0.4758\n",
            "Epoch 151/300\n",
            "10938/10938 [==============================] - 198s 18ms/step - d_loss: 0.5688 - g_loss: 0.4784\n",
            "Epoch 152/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5685 - g_loss: 0.4709\n",
            "Epoch 153/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5688 - g_loss: 0.4662\n",
            "Epoch 154/300\n",
            "10938/10938 [==============================] - 199s 18ms/step - d_loss: 0.5681 - g_loss: 0.4740\n",
            "Epoch 155/300\n",
            "10938/10938 [==============================] - 202s 18ms/step - d_loss: 0.5689 - g_loss: 0.4699\n",
            "Epoch 156/300\n",
            "10938/10938 [==============================] - 203s 19ms/step - d_loss: 0.5681 - g_loss: 0.4760\n",
            "Epoch 157/300\n",
            "10938/10938 [==============================] - 203s 19ms/step - d_loss: 0.5683 - g_loss: 0.4829\n",
            "Epoch 158/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5686 - g_loss: 0.4700\n",
            "Epoch 159/300\n",
            "10938/10938 [==============================] - 198s 18ms/step - d_loss: 0.5681 - g_loss: 0.4853\n",
            "Epoch 160/300\n",
            "10938/10938 [==============================] - 198s 18ms/step - d_loss: 0.5689 - g_loss: 0.4862\n",
            "Epoch 161/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5689 - g_loss: 0.4712\n",
            "Epoch 162/300\n",
            "10938/10938 [==============================] - 198s 18ms/step - d_loss: 0.5683 - g_loss: 0.4599\n",
            "Epoch 163/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5686 - g_loss: 0.4802\n",
            "Epoch 164/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5687 - g_loss: 0.4622\n",
            "Epoch 165/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5687 - g_loss: 0.4531\n",
            "Epoch 166/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5683 - g_loss: 0.4500\n",
            "Epoch 167/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5683 - g_loss: 0.4860\n",
            "Epoch 168/300\n",
            "10938/10938 [==============================] - 201s 18ms/step - d_loss: 0.5690 - g_loss: 0.4682\n",
            "Epoch 169/300\n",
            "10938/10938 [==============================] - 200s 18ms/step - d_loss: 0.5685 - g_loss: 0.4758\n",
            "Epoch 170/300\n",
            "10938/10938 [==============================] - 202s 19ms/step - d_loss: 0.5689 - g_loss: 0.4852\n",
            "Epoch 171/300\n",
            "10938/10938 [==============================] - 198s 18ms/step - d_loss: 0.5682 - g_loss: 0.4557\n",
            "Epoch 172/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5679 - g_loss: 0.4666\n",
            "Epoch 173/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5688 - g_loss: 0.4834\n",
            "Epoch 174/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5683 - g_loss: 0.4723\n",
            "Epoch 175/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5680 - g_loss: 0.4894\n",
            "Epoch 176/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5685 - g_loss: 0.4608\n",
            "Epoch 177/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5684 - g_loss: 0.4739\n",
            "Epoch 178/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5689 - g_loss: 0.4836\n",
            "Epoch 179/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5681 - g_loss: 0.4819\n",
            "Epoch 180/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5684 - g_loss: 0.4742\n",
            "Epoch 181/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5688 - g_loss: 0.4920\n",
            "Epoch 182/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5687 - g_loss: 0.4743\n",
            "Epoch 183/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5684 - g_loss: 0.4646\n",
            "Epoch 184/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5691 - g_loss: 0.4912\n",
            "Epoch 185/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5683 - g_loss: 0.4676\n",
            "Epoch 186/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5685 - g_loss: 0.4783\n",
            "Epoch 187/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5677 - g_loss: 0.4886\n",
            "Epoch 188/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5682 - g_loss: 0.4830\n",
            "Epoch 189/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5685 - g_loss: 0.4735\n",
            "Epoch 190/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5689 - g_loss: 0.4621\n",
            "Epoch 191/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5680 - g_loss: 0.4672\n",
            "Epoch 192/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5684 - g_loss: 0.4469\n",
            "Epoch 193/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5685 - g_loss: 0.4635\n",
            "Epoch 194/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5687 - g_loss: 0.4899\n",
            "Epoch 195/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5682 - g_loss: 0.4827\n",
            "Epoch 196/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5685 - g_loss: 0.4525\n",
            "Epoch 197/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5683 - g_loss: 0.4448\n",
            "Epoch 198/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5683 - g_loss: 0.4973\n",
            "Epoch 199/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5680 - g_loss: 0.4614\n",
            "Epoch 200/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5686 - g_loss: 0.4556\n",
            "Epoch 201/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5679 - g_loss: 0.4618\n",
            "Epoch 202/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5685 - g_loss: 0.4856\n",
            "Epoch 203/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5687 - g_loss: 0.5004\n",
            "Epoch 204/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5683 - g_loss: 0.4579\n",
            "Epoch 205/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5688 - g_loss: 0.4486\n",
            "Epoch 206/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5683 - g_loss: 0.4665\n",
            "Epoch 207/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5687 - g_loss: 0.4772\n",
            "Epoch 208/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5680 - g_loss: 0.4946\n",
            "Epoch 209/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5679 - g_loss: 0.4737\n",
            "Epoch 210/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5684 - g_loss: 0.4435\n",
            "Epoch 211/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5685 - g_loss: 0.4282\n",
            "Epoch 212/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5682 - g_loss: 0.4543\n",
            "Epoch 213/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5685 - g_loss: 0.4516\n",
            "Epoch 214/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5687 - g_loss: 0.4769\n",
            "Epoch 215/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5685 - g_loss: 0.4718\n",
            "Epoch 216/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5687 - g_loss: 0.4936\n",
            "Epoch 217/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5691 - g_loss: 0.4922\n",
            "Epoch 218/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5694 - g_loss: 0.4868\n",
            "Epoch 219/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5699 - g_loss: 0.4720\n",
            "Epoch 220/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5689 - g_loss: 0.4948\n",
            "Epoch 221/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5684 - g_loss: 0.4676\n",
            "Epoch 222/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5683 - g_loss: 0.4574\n",
            "Epoch 223/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5686 - g_loss: 0.4715\n",
            "Epoch 224/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5686 - g_loss: 0.4928\n",
            "Epoch 225/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5685 - g_loss: 0.4731\n",
            "Epoch 226/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5682 - g_loss: 0.4644\n",
            "Epoch 227/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5680 - g_loss: 0.4775\n",
            "Epoch 228/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5685 - g_loss: 0.4478\n",
            "Epoch 229/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5683 - g_loss: 0.4524\n",
            "Epoch 230/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5688 - g_loss: 0.4855\n",
            "Epoch 231/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5679 - g_loss: 0.4710\n",
            "Epoch 232/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5681 - g_loss: 0.4535\n",
            "Epoch 233/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5675 - g_loss: 0.4821\n",
            "Epoch 234/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5685 - g_loss: 0.4740\n",
            "Epoch 235/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5681 - g_loss: 0.4575\n",
            "Epoch 236/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5688 - g_loss: 0.4697\n",
            "Epoch 237/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5681 - g_loss: 0.4680\n",
            "Epoch 238/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5684 - g_loss: 0.4822\n",
            "Epoch 239/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5679 - g_loss: 0.4804\n",
            "Epoch 240/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5684 - g_loss: 0.4710\n",
            "Epoch 241/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5689 - g_loss: 0.4696\n",
            "Epoch 242/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5684 - g_loss: 0.4797\n",
            "Epoch 243/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5684 - g_loss: 0.4898\n",
            "Epoch 244/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5683 - g_loss: 0.4600\n",
            "Epoch 245/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5679 - g_loss: 0.4862\n",
            "Epoch 246/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5687 - g_loss: 0.4693\n",
            "Epoch 247/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5682 - g_loss: 0.4502\n",
            "Epoch 248/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5679 - g_loss: 0.4817\n",
            "Epoch 249/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5687 - g_loss: 0.4666\n",
            "Epoch 250/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5684 - g_loss: 0.4803\n",
            "Epoch 251/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5684 - g_loss: 0.4727\n",
            "Epoch 252/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5690 - g_loss: 0.4675\n",
            "Epoch 253/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5684 - g_loss: 0.4701\n",
            "Epoch 254/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5682 - g_loss: 0.4801\n",
            "Epoch 255/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5679 - g_loss: 0.4595\n",
            "Epoch 256/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5679 - g_loss: 0.4883\n",
            "Epoch 257/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5683 - g_loss: 0.4841\n",
            "Epoch 258/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5685 - g_loss: 0.4839\n",
            "Epoch 259/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5685 - g_loss: 0.4776\n",
            "Epoch 260/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5684 - g_loss: 0.4599\n",
            "Epoch 261/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5683 - g_loss: 0.4904\n",
            "Epoch 262/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5684 - g_loss: 0.4863\n",
            "Epoch 263/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5684 - g_loss: 0.4936\n",
            "Epoch 264/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5680 - g_loss: 0.4688\n",
            "Epoch 265/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5683 - g_loss: 0.4940\n",
            "Epoch 266/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5686 - g_loss: 0.4908\n",
            "Epoch 267/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5681 - g_loss: 0.4944\n",
            "Epoch 268/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5683 - g_loss: 0.4746\n",
            "Epoch 269/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5682 - g_loss: 0.4599\n",
            "Epoch 270/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5686 - g_loss: 0.4856\n",
            "Epoch 271/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5683 - g_loss: 0.4663\n",
            "Epoch 272/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5687 - g_loss: 0.4801\n",
            "Epoch 273/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5679 - g_loss: 0.4503\n",
            "Epoch 274/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5691 - g_loss: 0.4742\n",
            "Epoch 275/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5681 - g_loss: 0.4736\n",
            "Epoch 276/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5683 - g_loss: 0.4897\n",
            "Epoch 277/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5677 - g_loss: 0.4928\n",
            "Epoch 278/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5682 - g_loss: 0.4640\n",
            "Epoch 279/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5685 - g_loss: 0.4784\n",
            "Epoch 280/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5687 - g_loss: 0.4751\n",
            "Epoch 281/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5683 - g_loss: 0.4491\n",
            "Epoch 282/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5683 - g_loss: 0.4741\n",
            "Epoch 283/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5684 - g_loss: 0.4794\n",
            "Epoch 284/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5687 - g_loss: 0.4749\n",
            "Epoch 285/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5686 - g_loss: 0.4754\n",
            "Epoch 286/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5684 - g_loss: 0.4564\n",
            "Epoch 287/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5681 - g_loss: 0.4886\n",
            "Epoch 288/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5686 - g_loss: 0.4664\n",
            "Epoch 289/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5682 - g_loss: 0.4728\n",
            "Epoch 290/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5679 - g_loss: 0.4360\n",
            "Epoch 291/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5687 - g_loss: 0.4674\n",
            "Epoch 292/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5688 - g_loss: 0.4521\n",
            "Epoch 293/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5682 - g_loss: 0.4678\n",
            "Epoch 294/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5684 - g_loss: 0.4820\n",
            "Epoch 295/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5683 - g_loss: 0.4600\n",
            "Epoch 296/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5687 - g_loss: 0.4883\n",
            "Epoch 297/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5682 - g_loss: 0.4715\n",
            "Epoch 298/300\n",
            "10938/10938 [==============================] - 197s 18ms/step - d_loss: 0.5681 - g_loss: 0.4710\n",
            "Epoch 299/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5683 - g_loss: 0.4887\n",
            "Epoch 300/300\n",
            "10938/10938 [==============================] - 196s 18ms/step - d_loss: 0.5683 - g_loss: 0.4616\n"
          ]
        }
      ],
      "source": [
        "# print('inputs for GAN:',X)\n",
        "with tf.device('/device:GPU:0'):\n",
        "  hist = gan.fit(X,X,batch_size=BATCH,epochs=epochs_,verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 儲存weights\n",
        "generator.save_weights('./drive/MyDrive/model_weights/generator_weights',save_format='tf')\n",
        "\n",
        "discriminator.save_weights('./drive/MyDrive/model_weights/discriminator_weights',save_format='tf')"
      ],
      "metadata": {
        "id": "aGsQT5yuh7ay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHgazQyKdPE-"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load weights"
      ],
      "metadata": {
        "id": "GDbBZFbhK4Dy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generator = TransformerEncoderOnly(d_model,ffn_hidden,num_heads,drop_prob,num_layers,max_sequence_length,\n",
        "                        vocab_size,note_to_indx,note_to_indx,START_TOKEN,END_TOKEN,PADDING_TOKEN,delta_)\n",
        "discriminator = Discriminator(input_shape=(1,max_sequence_length))\n",
        "\n",
        "# 載入生成器和鑑別器的權重\n",
        "generator.load_weights('./drive/MyDrive/model_weights/generator_weights')\n",
        "discriminator.load_weights('./drive/MyDrive/model_weights/discriminator_weights')\n",
        "\n",
        "# 創建新的 GAN 模型\n",
        "predict_gan = TransformerGAN(generator,discriminator)\n",
        "\n",
        "# 梯度裁剪閥值\n",
        "clip_value = 1.0\n",
        "\n",
        "g_opt = Adam(learning_rate=1e-6,clipvalue=clip_value)\n",
        "d_opt = Adam(learning_rate=1e-6)\n",
        "g_loss = MeanSquaredError()\n",
        "d_loss = BinaryCrossentropy()\n",
        "predict_gan = TransformerGAN(generator,discriminator)\n",
        "\n",
        "predict_gan.compile(g_opt,d_opt,g_loss,d_loss)"
      ],
      "metadata": {
        "id": "keZMsyhIKuW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a sequence e.g. [0,0,0,...,0]"
      ],
      "metadata": {
        "id": "ucWgS931K9nR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rn6Yl0YAcHA6"
      },
      "outputs": [],
      "source": [
        "start = tf.zeros([1,128])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predict"
      ],
      "metadata": {
        "id": "IkAyMV6YLn0C"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nC3On4_kcGrk"
      },
      "outputs": [],
      "source": [
        "out = gan.generator(start,batch_=1)\n",
        "out = out.numpy()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run this if you don't have mido"
      ],
      "metadata": {
        "id": "z-SfdlWjLdAw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HAGb07ZU8MC2"
      },
      "outputs": [],
      "source": [
        "!pip install mido"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uTkF1HKhdC7P"
      },
      "outputs": [],
      "source": [
        "from mido import MidiFile , MidiTrack, Message"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a class generate midi with the output"
      ],
      "metadata": {
        "id": "XIWdJz0ALqjx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JohqTgWuc5ci"
      },
      "outputs": [],
      "source": [
        "class Midi_():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.mid = MidiFile()\n",
        "    self.track = MidiTrack()\n",
        "\n",
        "  def play_part(self,note, len_ ,note_bias=0,vel=1,delay=0,change=False,double=False):\n",
        "  # 每個節拍的時間長度\n",
        "\n",
        "    temple = 60*60*10/75\n",
        "    # 大調，參考別人的做法的，我也不是很懂樂理\n",
        "    major_notes = [0,2,2,1,2,2,2,1]\n",
        "    # C4 - 正中間的 DO(60)\n",
        "    base_note = note\n",
        "    # print(base_note)\n",
        "    bias = random.randint(-1,1)\n",
        "    vel = round(64*vel)\n",
        "    # delay = random.random()\n",
        "    t_start = round(delay*temple)\n",
        "    t_end =  round(temple*len_)\n",
        "\n",
        "    # if base_note < 128 and base_note > 0:\n",
        "        # base_note = base_note+bias\n",
        "\n",
        "\n",
        "    if not double:\n",
        "      self.track.append(Message(\"note_on\",  note=base_note,velocity=vel,time=t_start))\n",
        "      self.track.append(Message(\"note_off\", note=base_note,velocity=vel,time=t_end))\n",
        "    if change:\n",
        "      self.track.append(Message(\"control_change\",channel=0,control=64,value=64,time=t_start))\n",
        "      self.track.append(Message(\"control_change\",channel=0,control=64,value=0,time=t_end))\n",
        "    if double:\n",
        "      self.track.append(Message(\"program_change\", channel=1,program=41 ,time = t_start))\n",
        "      self.track.append(Message(\"note_on\",channel=1, note=base_note,velocity=vel,time = t_start))\n",
        "      self.track.append(Message(\"note_off\",channel=1, note=base_note,velocity=vel,time = t_end))\n",
        "      self.track.append(Message(\"program_change\", channel=1,program=0 ,time=t_end))\n",
        "    # return self.track\n",
        "\n",
        "  def make_file(self,notes,name='new_song.mid'):\n",
        "\n",
        "    for n in notes[0]:\n",
        "      self.play_part(int(n),0.5)\n",
        "\n",
        "    self.mid.tracks.append(self.track)\n",
        "\n",
        "    self.mid.save(name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C83PDx_0dVuP"
      },
      "outputs": [],
      "source": [
        "midi_encode = Midi_()\n",
        "midi_encode.make_file(notes=out,name = f'./drive/MyDrive/music_note_dataset/test16_2018_100.mid')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}